{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be8b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe28fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_PROFILE'] = \"332883119153_PowerUserAccess\"\n",
    "os.environ['AWS_DEFAULT_REGION'] = \"us-west-2\"\n",
    "\n",
    "#Creating Session With Boto3.\n",
    "session = boto3.Session(\n",
    "# aws_access_key_id='<your_access_key_id>',\n",
    "# aws_secret_access_key='<your_secret_access_key>'\n",
    ")\n",
    "\n",
    "#Creating S3 Resource From the Session.\n",
    "s3 = session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c13e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '1S9ABX1Q7K369EK4', 'HostId': 'Lnu3GCHhBa6DY4SerlNkHxePYLSbze5AGhmEZtFKVyRoadsgb13cTLz/BtDs0wLNcdyxat8z13c=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'Lnu3GCHhBa6DY4SerlNkHxePYLSbze5AGhmEZtFKVyRoadsgb13cTLz/BtDs0wLNcdyxat8z13c=', 'x-amz-request-id': '1S9ABX1Q7K369EK4', 'date': 'Tue, 02 Aug 2022 03:37:31 GMT', 'x-amz-server-side-encryption': 'AES256', 'etag': '\"8be215e8b070c4c4da758975cee864d2\"', 'server': 'AmazonS3', 'content-length': '0'}, 'RetryAttempts': 0}, 'ETag': '\"8be215e8b070c4c4da758975cee864d2\"', 'ServerSideEncryption': 'AES256'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "txt_data = b'This is the content of the file uploaded from python boto3 asdfasdf'\n",
    "\n",
    "bucket_name = \"kefei-test\"\n",
    "\n",
    "object = s3.Object(bucket_name, 'file_uploaded_by_boto3.txt')\n",
    "\n",
    "result = object.put(Body=txt_data)\n",
    "\n",
    "print(result)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822810c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Upload the file\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = \"kefei-test\"\n",
    "file_name = \"file_uploaded.txt\"\n",
    "object_name = file_name\n",
    "try:\n",
    "    response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    print(response)\n",
    "except ClientError as e:\n",
    "    logging.error(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ade4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cdaf28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0  11\n",
       "1   2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.DataFrame([11, 2])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "432e7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/64859257/upload-data-to-s3-bucket-without-saving-it-to-a-disk\n",
    "import io\n",
    "\n",
    "# try out puobject from directly from memory\n",
    "\n",
    "client = boto3.client('s3')\n",
    "\n",
    "bucket = \"kefei-test\"\n",
    "file_buffer = io.StringIO()\n",
    "df_test.to_csv(file_buffer)\n",
    "file_body_to_upload = file_buffer.getvalue()\n",
    "file_name_on_s3 = \"csv_test_s3.csv\"\n",
    "\n",
    "response = client.put_object(\n",
    "    Body=file_body_to_upload,\n",
    "    Bucket=bucket,\n",
    "    Key=file_name_on_s3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cf5b7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6F2H53D7P11HPCGR',\n",
       "  'HostId': 'thb7HpI1gQhZnlxrihFmYkvrskYvuQ3/7IUlIrPyU/g1Y/EkBTvSKY78KLfHPvgfJArNmIqGMoY=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'thb7HpI1gQhZnlxrihFmYkvrskYvuQ3/7IUlIrPyU/g1Y/EkBTvSKY78KLfHPvgfJArNmIqGMoY=',\n",
       "   'x-amz-request-id': '6F2H53D7P11HPCGR',\n",
       "   'date': 'Tue, 02 Aug 2022 04:38:42 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"9c14cf4199bcb13cfe4d2fa2a66ded13\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"9c14cf4199bcb13cfe4d2fa2a66ded13\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a47ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/64859257/upload-data-to-s3-bucket-without-saving-it-to-a-disk\n",
    "import io\n",
    "\n",
    "# try out puobject from directly from memory\n",
    "\n",
    "client = boto3.client('s3')\n",
    "\n",
    "bucket = \"kefei-test\"\n",
    "file_buffer = io.StringIO()\n",
    "df_test.to_csv(file_buffer)\n",
    "file_body_to_upload = file_buffer.getvalue()\n",
    "file_name_on_s3 = \"csv_test_s3.csv\"\n",
    "\n",
    "response = client.put_object(\n",
    "    Body=file_body_to_upload,\n",
    "    Bucket=bucket,\n",
    "    Key=file_name_on_s3,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
